{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data=[1,2,3,4,5]\n",
    "y_data=[1,2,3,4,5]\n",
    "\n",
    "w=tf.Variable(2.9)\n",
    "b=tf.Variable(0.5)\n",
    "#Variable 은 텐서를 메모리에 저장하는 변수 \n",
    "#텐서 : 스칼라와 백터를 일반화 한 것\n",
    "hypothesis=w*x_data +b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y_data))  #rank가 1인 벡터의 차로을 스칼라 즉, rank==0으로 줄인다는 개념에서 reduce를 붙임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|     2.452|     0.376|45.6600037\n",
      "   10|    1.1036| 0.0033983| 0.2063358\n",
      "   20|    1.0128| -0.020915| 0.0010261\n",
      "   30|    1.0065| -0.021845| 0.0000926\n",
      "   40|    1.0059| -0.021227| 0.0000827\n",
      "   50|    1.0057| -0.020527| 0.0000772\n",
      "   60|    1.0055| -0.019844| 0.0000722\n",
      "   70|    1.0053| -0.019183| 0.0000674\n",
      "   80|    1.0051| -0.018544| 0.0000630\n",
      "   90|     1.005| -0.017927| 0.0000589\n"
     ]
    }
   ],
   "source": [
    "W=tf.Variable(2.9)\n",
    "b=tf.Variable(0.5)\n",
    "learning_rate=0.01\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W*x_data +b\n",
    "        cost =tf.reduce_mean(tf.square(hypothesis-y_data))\n",
    "    W_grad, b_grad =tape.gradient(cost,[W,b])  #미분을 실시, 손실함수를 최소로 만들어 주는 W,b값을 찾는다. \n",
    "    W.assign_sub(learning_rate*W_grad)  # W 값 조정  \n",
    "    b.assign_sub(learning_rate*b_grad)  # b 값 조정, 학습률을 통해 기울기를 얼만큼 반영할 것인지 결정한다.\n",
    "    if i%10==0:\n",
    "        print(\"{:5}|{:10.5}|{:10.5}|{:10.7f}\".format(i,W.numpy(),b.numpy(),cost))\n",
    "        #f는 scientific notation으로 표기하지 않는다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 W,b\n",
    "초기에 추정치의 초기값을 임의로 정한다. \n",
    "추정치를 갱신시켜준다. (assign_sub 메소드 부분)\n",
    "\n",
    "점점 변수를 업데이트 할 수록 W는 1에 가까워지고, b는 0에 가까워지고 있다. 그에 따라 손실함수는 최소화 됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.0066934, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W*5+b)\n",
    "print(W*2.5+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
